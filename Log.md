# 100 Days Of ML - LOG

## Day 0 : August 30, 2020
 
**Today's Progress** : I have setup all the things I needed to complete this challenge (Hopefully).

**Thoughts** : Today was hectic. One thing led to another and it took me several hours. Hope this will be exciting, will help me in learning Machine Learning in a more effective way.

## Day 1 : August 31, 2020

**Today's Progress** : I started learning the basics of ML about surpervised learning and unsupervised learning, linear regression, clustering, etc.

**Thoughts** : It was really fun. Octave is a good platform to learn ML as a beginner. I put in extra efforts to google whatever I didn't understand by pausing the video at the moment itself and not keeping it for later.

## Day 2 : September 01, 2020

**Today's Progress** :  
1. Applications of square errored function, cost function, contour plots, computation of gradient descent.
1. Predicted multiple linear regression hypotheses simultaneously using matrices in Octave.
     
**Thoughts** : The entire process of analysing both gradient descent and linear regression individually and then implementing the batch gradient descent algorithm to the linear regression model was really fascinating and fun. Things seem to be really obvious once you understand them truly.

## Day 3 : September 02, 2020

**Today's Progress** :  
1. Completed onramp MATALB course about the basics of MATLAB (matrix manipulations, plotting of functions and data, implementation of algorithms), importing and visualizing data and implemented it in projects.
1. The projects made were-
   1. Electricity usage and prices- Project to plot electricity usage and prices for various economic sectors.
   1. Audio frequency- Project to plot a signal that contains beat phenomenon and then analyze the signal's frequency content.
   1. Stellar motion-  Project to find and determine how fast the star is moving away from earth by using the wavelength characteristic spectrum of star.
   
**Thoughts** : Getting to use MATLAB a multi-paradigm numerical computing environment and proprietary programming language was fun. Using MATLAB to predict stuff in real life examples was really fascinating.

**Link of Work:**  [Day3_Projects](https://github.com/kritanjalijain/Blog--100_Days_0f_ML/tree/master/Day3_Projects)

## Day 4 : September 03, 2020

**Today's Progress** :  
1. Implemented multivariant linear regression and predicted form of hypothesis when it has multiple features.
   1. Learnt how to fit the parameters of that hypothesis. In particular, about how to use gradient descent for linear regression with multiple features.
   1. Applications of feature scaling and mean normalization to gradient descent, debugging gradient descent, automatic convergence test and choosing a suitably small learning rate.
   1. Implementation of normal equation vs gradient descent.
1. Predicted cost function using polynomial regression.
1. Apart from octave and MATLAB, I scratched the surface of ML using python libraries like Pandas, Numpy and OpenCV.
     
**Thoughts** : Today was a bit overwhelhming but exciting. I'm really eager to dive deeper into machine learning.

## Day 5 : September 04, 2020

**Today's Progress** :  
1. Project- Predict the profits of a restaurant using linear regression from batch gradient descent algorithm and plot it's contour plot and surface plot.
   
**Thoughts** : The entire process of analysing both gradient descent and linear regression individually and then implementing the batch gradient descent algorithm to the linear regression model was really fascinating and fun. Things seem to be really obvious once you understand them truly.

**Link of Work:**  [Day5_Projects](https://github.com/kritanjalijain/Blog--100_Days_0f_ML/tree/master/Day5_Projects)

## Day 6 : September 05, 2020

**Today's Progress** :  
1. Project- Predict and plot the prices of houses using multivariant linear regression, from both normal equation and gradient descent algorithm.
1. Explored Numpy.
   
**Thoughts** : Visualizing data really helps understand the model better.

**Link of Work:**  [Day6_Projects](https://github.com/kritanjalijain/Blog--100_Days_0f_ML/tree/master/Day6_Projects)

## Day 7 : September 07, 2020

**Today's Progress** :  
 Classification problems - 
   * Hypothesis representation, visualisation of logistic regression; decision boundary.
   
**Thoughts** : Finally got to know applications of the logistic function or the sigmoid function which I studied in 11th grade. I can finally understand the real life applications of matrices, algebra and calculus studied previously. Well, this is fun :)

## Day 8 : September 08, 2020

**Today's Progress** :  
 Classification problems -
   * Derivation of cost function for logisitic regression
   * Gradient descent for minimizing the cost function J of theta for logistic regression.
  
 Basics of sophisticated optimization algorithms like-
  * Conjugate gradient 
  * BFGS 
  * L-BFGS 

## Day 9 : September 09, 2020

**Today's Progress** :  
 Multi-Classification problems -
   * One vs All
   * Derivation of cost function for logisitic regression
   * Gradient descent for minimizing the cost function J of theta for logistic regression.

## Day 10 : September 10, 2020

**Today's Progress** :  
   * Regularization to help ameliorate models from overfitting the training data.(Since machine learning models need to generalize well to new examples that the model has not seen in practice.)
   
**Thoughts** : Choosing a model with an appropriate algorithm is essential and an algorithm fiiting the data too perfectly (over-fitting) is not appropriate at times since it has high variance just like underfitting has high bias. 

## Day 11 : September 11, 2020

**Today's Progress** :  
   * Ridge regularization
   * Lasso regularization

**Link of Work** : [Day11_Projects](https://github.com/kritanjalijain/100_Days_0f_ML/tree/master/Day11_Projects)

## Day 12 : September 12, 2020

**Today's Progress** :  
   * Using MATLAB's Classification Learner app to train models of logistic regression to make predictions.
   
**Thoughts** : MATLAB's inbuilt apps really make it easier to train models.

**Link of Work** : [Day12_Projects](https://github.com/kritanjalijain/100_Days_0f_ML/tree/master/Day12_Projects)

## Day 13 : September 13, 2020

**Today's Progress** :  
   * Understanding the need/intiution for Neural Networks
   
**Thoughts** : One of the reasons they excite me is that maybe they give us this window into what we might do if we're also thinking of what algorithms might someday be able to learn in a manner similar to humankind. 

## Day 14 : September 14, 2020

**Today's Progress** :  
   * Neural Networks model representations, mathematical hypotheses representation and vectorized implementations

## Day 15 : September 15, 2020

**Today's Progress** :  
   * Neural Networks to compute a complex non linear function of the input.
   
**Thoughts** : The first hidden layer computes some set of features from the input. The next hidden layer computes even more complex features and even more complex features. And these features can then be used by essentially the final layer of the logistic classifiers to make accurate predictions without the numbers that the network sees. 

## Day 16 : September 16, 2020

**Today's Progress** :  
   * Neural Networks in multiclass classifications
   * Neural Networks in binary classifications

**Link of Work** : [Day16_Projects](https://github.com/kritanjalijain/100_Days_0f_ML/tree/master/Day16_Projects)

## Day 17 : September 17, 2020

**Today's Progress** :  
   * Implemented a multi-class (one-vs-all) logistic regression model using the `fitcecoc` function from the Statistics and Machine Learning Toolbox of MATLAB. 
   * Created a pre trained neural network using tools from the Deep Learning Toolbox which is used to classify handwritten digits.

**Link of Work** : [Day17_Projects](https://github.com/kritanjalijain/100_Days_0f_ML/tree/master/Day17_Projects)

## Day 18 : September 18, 2020

**Today's Progress** :  
   * Training Neural Networks
   * Algorithm for fitting parameters and optimizing the cost function
   * Forward Propagation algorithm

## Day 19 : September 19, 2020

**Today's Progress** :  
* Backpropagation algorithm
* Implementation of appropriate network architecture for learning model
   
**Thoughts** : Its really fascinating to see the various applications of neural networks like autonomous driving, how the algorithm learns itself and responds under different variables.

## Day 20 : September 20, 2020

**Today's Progress** :  
   * Unrolling parameters to vectorize them in order to use advanced optimization routines
   * Using gradient checking for debugging backpropagation algorithms
   * Random initializaton for symmetry breaking

## Day 21 : September 21, 2020

**Today's Progress** :  
   * Implementing backpropagation algorithm on neural network for handwritten digit recognition.
   
**Thoughts** : Back propagation is a very complicated algorithm and will be able to fit very complex, powerful, non-linear functions to the data, and this is one of the most effective learning algorithms we have today. 

**Link of Work** : [Day21_Projects](https://github.com/kritanjalijain/100_Days_0f_ML/tree/master/Day21_Projects)

## Day 22 : September 22, 2020

**Today's Progress** :  
   * Implementing backpropagation algorithm on neural network for handwritten digit recognition.

**Link of Work** : [Day22_Projects](https://github.com/kritanjalijain/100_Days_0f_ML/tree/master/Day22_Projects)

## Day 23 : September 23, 2020

**Today's Progress** :  
   * Updated pending github READMEs and organized repo
   
**Thoughts** : 

## Day 24 : September 24, 2020

**Today's Progress** :  
   * Learning about systematically improving the learning algorithm i.e. how to evaluate a hypotheses, select the training model and validation sets and test sets.
   * Learning on how to tell when a learning algorithm is doing poorly, and describe the 'best practices' for how to 'debug' the learning algorithm and go about improving its performance. 
   
**Thoughts** : 

## Day 25 : September 25, 2020

**Today's Progress** :  
   * Machine learning system design
   * High bias and variance
   * Test and cross validation
   
**Thoughts** : 

## Day 26 : September 26, 2020

**Today's Progress** :  
   * k-folds
   * spam classifier
   
**Thoughts** : 

## Day 27 : September 27, 2020

**Today's Progress** :  
   * SVM algorithm
   
**Thoughts** : SVMs are considered by many to be the most powerful 'black box' learning algorithm, and by posing a cleverly-chosen optimization objective, one of the most widely used learning algorithms today. 

## Day 28 : September 28, 2020

**Today's Progress** :  
   * Kernels for complex non-linear functions
   * Similarity and landmarks
   
**Thoughts** : 

## Day 29: September 29, 2020

**Today's Progress** :  
   * Kernels
   
**Thoughts** : 
 * 
 
 **Thoughts** : 

## Day 30: September 30, 2020

**Today's Progress** :  
   * SVM Application
   
**Thoughts** : 
 * 
## Day 31: October 3, 2020

**Today's Progress** :  
   * 
   
**Thoughts** : 
 * 
 * 
## Day 32: October 04, 2020

**Today's Progress** :  
   * This lab will give you hands-on practice with TensorFlow 2.x model training, both locally and on AI Platform. After training, you will learn how to deploy your model to AI Platform for serving (prediction). You'll train your model to predict income category of a person using the United States Census Income Dataset.

This lab gives you an introductory, end-to-end experience of training and prediction on AI Platform. The lab will use a census dataset to:

    Create a TensorFlow 2.x training application and validate it locally.
    Run your training job on a single worker instance in the cloud.
    Deploy a model to support prediction.
    Request an online prediction and see the response.

What you will build

The sample builds a classification model for predicting income category based on United States Census Income Dataset. The two income categories (also known as labels) are:

    >50K — Greater than 50,000 dollars
    <=50K — Less than or equal to 50,000 dollars

The sample defines the model using the Keras Sequential API. The sample defines the data transformations particular to the census dataset, then assigns these (potentially) transformed features to either the DNN or the linear portion of the model.
   
**Thoughts** : 
 * 

## Day 33: October 05, 2020

**Thoughts** : 


## Day 34: October 06, 2020

**Thoughts** : 

## Day 35: October 07, 2020

**Thoughts** : 

## Day 36: October 08, 2020

**Thoughts** : 

## Day 37: October 09, 2020

**Thoughts** : 

## Day 38: October 10, 2020

**Thoughts** : 

## Day 39: October 11, 2020

* Unsupervised Learning - Clustering
  1. K means algorithm
      * Optimization objective
      * Random Initialization
      * Number of clusters K

**Thoughts** : 

## Day 40: October 12, 2020

* Dimensionality reduction -
   * Data compression
   * Data visualization
 * Principal Component analysis -
   * Foundation
   * Problem formulation
   * Algorithm

## Day 41: October 13, 2020

 * Reconstruction of original high dimensional data from compressed representation
 * Choosing number of prinicipal components k
 * Principal Component analysis applications
 * Speed up running time of learning algorithms using PCA

## Day 42: October 14, 2020

 * Density Estimation
 * Multivariant gaussian distribution
 * Anomaly Detection system applications, algorithm
 
## Day 43: October 15, 2020

 * Formalism of a recommendation system problem
 * Content based recommendation system -
     * Optimization objective
     * Optimization algorithm
  * Collaborative Filtering
 
**Thoughts** : 

## Day 44: October 16, 2020

  * Collaborative Filtering Algorithm
  * Low Rank Matrix Factorization on recommendation systems
 
**Thoughts** : 

## Day 45: October 17, 2020

  * Mean Normalization implemention on recommendation systems
 
**Thoughts** : 

## Day 46: October 18, 2020

  * Stochatic gradient descent on larger datasets
  * A way for approximately monitoring how the stochastic gradient descent is doing in terms for optimizing the cost function and its convergence behaviour
  * Mini batch gradient descent on larger datasets
  * Map reduce and data parallelism 
  * Introduction to online learning algorithms for continuous stream of data

## Day 47: October 19, 2020

  * Buliding pipeline
      * Text Detection 
      * Character segmentation
      * Text Classification and character recognition
  * Sliding windows object classification for computer vision 
  * Artifical Data Synthesis 
    * Using distortions
  * Ceiling Analysis
  
 ## Day 48: October 20, 2020

  * Logisitc regression model which is a cat classifier 
  * GCP lab
  
 ## Day 49: October 21, 2020

  * GCP lab

 ## Day 50: October 22, 2020

  * GCP lab
  
 ## Day 51: October 23, 2020

  * Pandas 
  * Structuring ML models
  
  ## Day 52: October 26, 2020

  * Pandas 
  * CNN
  
  ## Day 53: October 27, 2020
  
  

